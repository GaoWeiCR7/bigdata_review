# 高级MapReduce编程技术
* 复合键值对的使用 
1. 用复合键让系统完成排序
    * map计算过程结束后进行Partitioning处理时，系统自动按照map的输出键进行排序，进入Reduce节点的key有序，value无序
    * 将value中需要排序的部分加入到key中形成复合键，这样将能利用MapReduce系统的排序功能完成排序。 
    * 需要实现一个新的Partitioner，保证原来同一key值的键值对最后分区到同一个Reduce节点上
    * 样例：课件带频率的倒排索引
2. 把小的键值对合并成大的键值对 
    * 通常一个计算问题会产生大量的键值对，为了减少键值对传输和排序的开销，一些问题中的大量小的键值对可以被合 并成一些大的键值对(pairs->stripes)
    * 样例：课件单词同现矩阵算法 
* 用户自定义数据类型 
1. Hadoop内置的数据类型 
    * 这些数据类型都实现了WritableComparable接口，以便 进行网络传输和文件存储，以及进行大小比较
2. 用户自定义数据类型 
    * 需要实现Writable接口，作为key或者需要比较大小时则需要实现WritableComparable接口 
```
public class Point3Dimplements WritableComparable <Point3D> {   
    private int x, y, z; 
    public  int getX() {  
        return x; 
    } 
    public  int getY() {  
        return y; 
    } 
    public  int getZ() {  
        return z;
    } 
    public void write(DataOutput out) throws IOException {  
        out.writeFloat(x); 
        out.writeFloat(y); 
        out.writeFloat(z); 
    } 
    public void readFields(DataInput in) throws IOException {  
        x = in.readFloat(); 
        y = in.readFloat(); 
        z = in.readFloat(); 
    } 
    public int compareTo(Point3D p) {  
        //compares this(x, y, z) with p(x, y, z) and outputs -1, 0, 1    
    } 
}
```
```
public class Edge implements WritableComparable<Edge> { 
    private String departureNode; 
    private String arrivalNode; 
    public String getDepartureNode() { 
        return departureNode;
    }
    @Override public void readFields(DataInput in) throws IOException {   
        departureNode = in.readUTF(); 
        arrivalNode = in.readUTF(); 
    } 
    @Override 
    public void write(DataOutput out) throws IOException {   
        out.writeUTF(departureNode); 
        out.writeUTF(arrivalNode); 
    } 
    @Override 
    public int compareTo(Edge o) {   
        return (departureNode.compareTo(o.departureNode)!=0) ?departureNode.compareTo(o.departureNode):arrivalNode.compareTo(o.arrivalNode); 
    } 
}
```
* 用户自定义输入输出格式 
1. Hadoop内置的文件输入格式
    * TextInputFormat：默认，Key:The byte offset of the line; Value: The line contents
    * KeyValueTextInputFormat: Key:Everything up to the first tab character;Value: The remainder of the line
2. Hadoop内置的RecordReader
    * LineRecordReader: default reader for TextInputFormat; reads lines of text files
    * KeyValueLineRecordReader: default reader for KeyValueTextInputFormat; parses lines into keyvalpairs
3. 用户自定义InputFormat和RecordReader示例
```
public class FileNameOffsetInputFormat extends FileInputFormat<Text, Text> {
    @Override 
    public RecordReader<Text, Text> createRecordReader(InputSplit split, TaskAttemptContext context) {   
        FileNameOffsetRecordReaderfnrr = new FileNameOffsetRecordReader(); try {   
            fnrr.initialize(split, context); 
            } catch (IOException e) {e.printStackTrace(); } 
            catch (InterruptedException e) {  e.printStackTrace(); } return fnrr; 
    } 
}
```
```
public class FileNameOffsetRecordReaderextends RecordReader<Text, Text> {
    String fileName; 
    LineRecordReader lrr = new LineRecordReader();
     …… 
     @Override 
     public Text getCurrentKey() throws IOException, InterruptedException {
        return new Text("(" + fileName + “#" + lrr.getCurrentKey() + ")"); 
    } 
    @Override 
    public Text getCurrentValue() throws IOException, InterruptedException {
       return lrr.getCurrentValue(); 
    } 
    @Override
     public void initialize(InputSplit arg0, TaskAttemptContext arg1) throws IOException, InterruptedException {   
        lrr.initialize(arg0, arg1); 
        fileName = ((FileSplit)arg0).getPath().getName(); 
    } 
}
```
```
job.setInputFormatClass(FileNameOffsetInputFormat.class); 
```

4. Hadoop内置的OutputFormat和RecordWriter
    * TextOutputFormat : Default; writes lines in "key \t value" form
    * LineRecordWriter : Default RecordWriter for TextOutputFormat writes lines in "key \t value" form
5. 与InputFormat和RecordReader类似，用户可以根据需要定制 OutputFormat和RecordWriter

* 用户自定义Partitioner和Combiner 
1. 定制Partitioner : 程序员可以根据需要定制Partitioner来改变Map中间结果到Reduce 节点的分区方式，并在Job中设置新的Partitioner 
```
Class NewPartitioner extends HashPartitioner<K,V> {   
    // override the method getPartition(Kkey, Vvalue, intnumReduceTasks) { 
        term = key.toString().split(“,”)[0]; //<term, docid>=>term 
        super.getPartition(term, value, numReduceTasks); 
    } 
} 
并在Job中设置新的Partitioner： 
Job. setPartitionerClass(NewPartitioner)
```
2. 定制Combiner : 程序员可以根据需要定制Combiner来减少网络数据传输量，提高系统效率，并在Job中设置新的Combiner 
```
public static class NewCombiner extends Reducer < Text, IntWritable, Text, IntWritable > { 
    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException { 
        // 忽略(<year, country>, [1, 1,1,…])后部很长的数据串， 
        // 归并为<year, country>的1次出现 
        context.write(key, new IntWritable(1));  
    } 
    // 输出key: <year, country>；value: 1 
}
并在Job中设置新的Combiner： 
Job. setCombinerClass(NewCombiner)

Combiner输出时不能改变其输入键值对的格式，必须保持一致！ 
```
* 迭代MapReduce和组合式MapReduce程序设计见课件，不难
* 多数据源的连接 
1. 用文件复制实现Map端Join 
    * 用distributed cache将一个或多个小数据量文件分布复制到所有节点上 
    * Job类中：public void addCacheFile(URI uri) 将一个文件放到distributed cache file中 
    * Mapper或Reducer的context类中： public Path[] getLocalCacheFiles() 获取设置在distributed cache files中的文件路径 
    * 样例见课件
2. 以上多数据源连接解决方法的限制 : 
    * 以上的多数据源Join只能是具有相同主键/外键的数据源间的连接，如果数据源两两之间具有多个不同的主键/外键的连接，则需要使用多次MapReduce过程完成不同主/外键间的连接。 如，有三个数据源：Customers（CustomerID）， Orders（CustomerID，ItemID），以及Products(ItemID) 
    * 在MapReduce中将需要分两个MapReduce作业来完成三个数据源的Join:第一个MapReduce作业完成Customers与Orders的Join，然后，Join后的结果再通过第二个MapReduce作业完成与Products的Join
* 全局参数/数据文件的传递 
1. 全局作业参数的传递
    * Configuartion类专门提供以下用于保存和获取属性的方法： 
    * public void set(String name, String value) //设置字符串属性 
    * public String get(String name) // 读取字符串属性 
    * public String get(String name, String defaultValue)   // 读取字符串属性 
    * public void setBoolean(String name, boolean value) //设置布尔属性  
    * public boolean getBoolean(String name, boolean defaultValue) //读取布尔属性 
    * public void setInt(String name, int value) //设置整数属性 
    * public int getInt(String name, int defaultValue)    // 读取整数属性 
    * public void setLong(String name, long value) //设置长整数属性 
    * public long getLong(String name, long defaultValue)  // 读取长整数属性 
    * public void setFloat(String name, float value) //设置浮点数属性 
    * public float getFloat(String name, float defaultValue) //读取浮点数属性 
    * public void setStrings(String name, String... values) //设置一组字符串属性 
    * public String[] getStrings(String name, String... defaultValue) //读取一组字符串属性
    * 需要说明的是，setStrings方法将把一组字符串转换为用“，”隔开的一个长字符串，然后getStrings时自动再根据“,”split成一组字符串，因此，在该组中的每个字符串都不能包含“，”，否则会出错。
    * 例：专利文献数据集Join时主键所在数据列参数的设置 
```
Configuration jobconf = new Configuration(); 
Job job = new Job(jobconf, MyJob.class); ... 
// 将第三个输入参数设置为JoinKeyColIdx属性 
jobconf.setInt(“JoinKeyColIdx”, Integer.parseInt(args[2]));
…… 
Job.waitForCompletion(true);
```
```
在mapper类的初始化方法setup()中从configuration对象中读出属性 
public static class MapClass extends Mapper<Text, Text, Text, Text> {
    int join_key_col_idx; 
    public void setup(Mapper.Contextcontext) { 
        Configuration jobconf= context.getConfiguration()； 
        join_key_col_idx = jobconf.getInt(“JoinKeyColIdx”, -1); 
        // 无值时置为-1 
    } 
    protected void map(Text key, Text value, Context context)  throws IOException, InterruptedException {   
        //使用join_key_col_idx完成数据处理； …… 
    } 
}
```
```
同样需要时在reducer类的初始化方法setup()中从configuration对象中读出属性 
public static class ReduceClass extends Reducer<Text, Text, Text, Text> {
    int join_key_col_idx; 
    public void setup(Mapper.Contextcontext) { 
        Configuration jobconf= context.getConfiguration()； 
        join_key_col_idx = jobconf.getInt(“JoinKeyColIdx”, -1); 
        // 无值时置为-1 
    } 
    protected void reduce(Text key, Text value, Context context)  throws IOException, InterruptedException {   
        //使用join_key_col_idx完成数据处理； …… 
    } 
}
```
* 其它处理技术见课件
