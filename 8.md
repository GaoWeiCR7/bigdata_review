# 基于MapReduce的数据挖掘基础算法
* 基于MapReduce的K-Means并行化算法
1. K-Means聚类算法介绍
    * 将给定的多个对象分成若干组，组内的各个对象是相似的，组间的对象是不相似的。进行划分的过程就是聚类过程，划分后的组称为簇(cluster)。
```
输入：待聚类的N个数据点，期望生成的聚类的个数K 输出：K个聚类 
算法描述: 
选出K个点作为初始的cluster center 
Loop: 
对输入中的每一个点p：
｛ 
计算p到各个cluster的距离；
将p归入最近的cluster; 
｝ 
重新计算各个cluster的中心
如果不满足停止条件，goto Loop; 否则，停止
```
2. 样例过程见课件
3. K-Means是个不断迭代的过程
    * 第i轮迭代 : 生成新的clusters，并计算cluster centers
    * 第i+1轮迭代 : 根据第i轮迭代中生成的clusters和计算出的cluster centers，进行新一轮的聚类
    * 如此不断迭代直到满足终止条件
4. K-Means算法的局限性
    * 对初始cluster centers的选取会影响到最终的聚类结果
    * 由此带来的结果是：能得到局部最优解，不保证得到全局最优解
    * 相似度计算和比较时的计算量较大
5. 每一个节点需要访问如下的全局文件 
    * 当前的迭代计数 
    * K个表示不同聚类中心的如下的数据结构
    * cluster id ; cluster center ; 属于该cluster  center的数据点的个数
6. Map阶段的处理
    * 在Map类的初始化方法setup中读取全局的聚类中心信息 
    * 对Map方法收到的每一个数据点p，计算p与所有聚类中心间的距离，并选择一个距离最小的中心作为p所属的聚类，输出<ClusterID,(p,1)>键值对
    * 对每个Map节点上即将传递到Reduce节点的每一个 <ClusterID,(p,1)>键值对，用Combiner进行数据优化，合并相同ClusterID下的所有数据点并求取这些点的均值pm以及 数据点个数n
```
Mapper伪代码 
class Mapper setup(…) { 
    读出全局的聚类中心数据 -> Centers 
}
map(key, p) // p为一个数据点 { 
    minDis = Double.MAX VALUE; 
    index = -1; 
    for i=0 to Centers.length { 
        dis= ComputeDist(p, Centers[i]); 
        if dis < minDis {   
            minDis = dis; 
            index = i; 
        } 
    } 
    emit(Centers[i].ClusterID, (p,1)); 
} 
```
```
Combiner伪代码 
class Combiner reduce(ClusterID, [(p1,1), (p2,1), …]) { 
    pm = 0.0； 
    n = 数据点列表[(p1,1), (p2,1), …]中数据点的总个数; 
    for i=0 to n 
        pm += p[i]; 
    pm = pm / n;  // 求得这些数据点的平均值 
    emit(ClusterID, (pm, n)); 
}
```
7. Reduce阶段的处理
    * 经过Map和Combine后从Map节点输出的所有ClusterID相同的中间结果<ClusterID, [(pm1, n1), (pm2, n3)…]>,计算新的均值 pm，输出<ClusterID, pm> 
    * 所有输出的<ClusterID, (pm,n)>形成新的聚类中心，供下一次迭代计算
    * 用不用Combiner仅仅会影响性能，不能改变计算结果。因此，Combiner输出时不允许改变Map输出键值对中Value的格式和类型，否则会出错
```
Reducer伪代码 
class Reducer reduce(ClusterID, value = [(pm1,n1),(pm2,n2) …]) { 
    pm = 0.0； n=0; 
    k = 数据点列表中数据项的总个数; 
    for i=0 to k {    
        pm += pm[i]*n[i];  
        n+= n[i]; 
    } 
    pm = pm / n;  
    // 求得所有属于ClusterID的数据点的均值 emit(ClusterID, (pm,n)); // 输出新的聚类中心的数据值 
} 
```

8. 终止迭代
    * 设定迭代次数; 
    * 均方差的变化（非充分条件） 
    * 每个点固定地属于某个聚类 
    * 其他设定条件 ... ... 
    * 与具体的应用高度相关

9. 聚类算法应用实例
    * Netflix公司与大奖赛

* 基于MapReduce的分类并行算法
1. 分类算法的基本作用是：从一组已经带有分类标记的训练样本数据集来预测一个测试样本的分类结果。 
2. 分类算法的基本描述是：一个训练数据集TR = {tr1，tr2, tr3,…} 每个训练样本tri是一个三元组（tid，A，y） 其中tid是每个训练样本的标识符，A是一组特征属性值：A = {a1，a2，a3,…}， 而y是训练样本已知的分类标记。
3. KNN算法：计算测试样本 到各训练样本的距离，取其中距离最小的K个，并根据这K个 训练样本的标记进行投票得到测试样本的标记。
4. KNN MapReduce并行化算法设计思路 
    * 将测试样本数据分块后分布在不同的节点上进行处理，将训练样本数据文件放在DistributedCache中 供每个节点共享访问 
    * Map阶段对每个读出的测试样本数据ts(trid,A’,y’），计算其与每个训练样本数据tr(trid,A,y)之间的相似度S=Sim(A’,A）（1：相似度最大，0：相似度最小），检查S是否比目前的k个S值中最小的大，若是则将(S,y)计入k个最大者，根据所保留的k个S值最大的(S,y)，根据模型y’=∑Si*yi/∑Si计算出ts的分类标记值y’，发射出(tsid, y’) 
    * Reduce阶段直接输出(tsid, y’)
```
Mapper伪代码 
class Mapper 
setup(…) { 
    读取全局训练样本数据文件，转入本地内存的数据表TR中 
} 
map(key, ts) // ts为一个测试样本 {  
    Φ -> MaxS (k); 
    ts -> tsid, A’, y’; 
    for i=0 to TR.lenghth {   
        TR[i] -> trid, A, y; 
        S = Sim(A, A’); 
        若S属于k个最大者， (S, y) -> MaxS; 
    }  
    根据MaxS和带加权投票表决模型计算出y’ =∑Si*yi/∑Si 
    emit(tsid, y’) 
}
```


