# Google /Hadoop MapReduce基本构架 
* Google MapReduce的基本工作原理 
1. Google MapReduce并行处理的基本过程(见课件)
2. 失效处理 : 
    * 主节点失效 : 周期性地设置检查点,一旦某个任务失效，可以从最近有效的检查点开始重新执行
    * 工作节点失效 : ，主节点会周期性地给工作节点发送心跳检测，如果工作节点没有回应，这认为该工作节点失效，主节点将终止该工作节点的任务并把失效的 任务重新调度到其它工作节点上重新执行
3. 带宽优化 :
    * 问题 : 大量的键值对数据在传送给Reduce节点时会引起较大的通信带宽开销。 
    * 解决方案 : 每个Map节点处理完成的中间键值队将由combiner做一个合并压缩，即把那些键名相同的键值对归并为一个键名下的 一组数值。
4. 计算优化 :
    * 问题 : Reduce节点必须要等到所有Map节点计算结束才能开始执行，因此，如果有一个计算量大、或者由于某个问题导致很慢结束的Map节点，则会成为严重的“拖后腿者”。
    * 解决方案 : 把一个Map计算任务让多个Map节点同时做，取最快完成者的计算结果。
5. 用数据分区解决数据相关性问题 
    * 问题 : 一个Reduce节点上的计算数据可能会来自多个Map节点，因此，为了在进入Reduce节点计算之前，需要把属于一个Reduce节点的数据归并到一起。 
    * 解决方案 : 在Map阶段进行了Combining以后，可以根据一定的策略对Map输出的中间结果进行分区(partitioning)，这样即可解决以上数据相关性问题避免Reduce计算过程中的数据通信。
* 分布式文件系统GFS的基本工作原理 
1. Google GFS的基本设计原则 
    * 廉价本地磁盘分布存储 : 各节点本地分布式存储数据，优点是不需要采用价格较贵的集中式磁盘阵列，容量可随节点数增加自动增加 
    * 多数据自动备份解决可靠性 : 采用廉价的普通磁盘，把磁盘数据出错视为常态，用自动多数据备份存储解决数据存储可靠性问题 
    * 为上层的MapReduce计算框架提供支撑 : GFS作为向上层MapReduce执行框架的底层数据存储支撑，负责处理所有的数据自动存储和容错处理，因而上层框架不需要考虑低层的数据存储和数据容错问题
2. 分布式结构化数据表BigTable(BigTable和GFS详细见课件)
    * BigTable的基本作用和设计思想 : BigTable提供了一定粒度的结构化数据操作能力，主要解决一些大型媒体数据（Web文档、图片等）的结构化存储问题。但与传统的关系数据库相比，其结构化粒度没有那么高，也没有事务处理等能力，因此，它并不是真正意义上的数据库
    * BigTable设计动机 : 
    1. 需要存储多种数据 : Google提供的服务很多，需处理的数据类型也很多，如URL,网页,图片,地图数据,email,用户的个性化设置等 
    2. 海量的服务请求 : Google是目前世界上最繁忙的系统之一，因此，需要有高性能的请求和数据处理能力 
    3. 商用数据库无法适用 : 在如此庞大的分布集群上难以有效部署商用数据库系统，且其难以承受如此巨量的数据存储和操作需求
    * BigTable主要设计目标 :
    1. 广泛的适用性:为一系列服务和应用而设计的数据存储系统,可满足对不同类型数据的存储和操作需求 
    2. 很强的可扩展性:根据需要可随时自动加入或撤销服务器节点 
    3. 高吞吐量数据访问:提供PB级数据存储能力，每秒数百万次的访问请求 
    4. 高可用性和容错性:保证系统在各种情况下度能正常运转，服务不中断 
    5. 自动管理能力：自动加入和撤销服务器，自动负载平衡 
    6. 简单性：系统设计尽量简单以减少复杂性和出错率
    * BigTable数据模型—多维表 :
    1. 一个行关键字(row key) 
    2. 一个列关键字(column key) 
    3. 一个时间戳(time stamp) 
* Hadoop 分布式文件系统HDFS 
1. HDFS的基本特征 ：
    * 存储极大数目的信息（terabytes or petabytes），将数据保存到大量的节点当中；支持很大的单个文件
    * 提供数据的高可靠性和容错能力，单个或者多个节点不工作，对系统不会造成任何影响，数据仍然可用。通过一定数量的数据复制保证数据存储的可靠性和出错恢复能力。
    * 与GFS类似，HDFS是MapReduce的底层数据存储支撑，并使得数据尽可能根据其本地局部性进行访问与计算。
    * HDFS对顺序读进行了优化，支持大量数据的快速顺序读出，代价是对于随机的访问负载较高。 
    * 数据支持一次写入，多次读取；不支持已写入数据的更新操作，但允许在文件尾部添加新的数据 
    * 数据不进行本地缓存（文件很大，且顺序读没有局部性） 
    * 基于块的文件存储，默认的块的大小是64MB : 有利于顺序读写（在磁盘上数据顺序存放） 
    * 多副本数据块形式存储，按照块的方式随机选择存储节点，默认副本数目是3
2. HDFS基本构架和HDFS数据分布设计见课件
3. HDFS可靠性与出错恢复 ：
    * DataNode节点的检测 : NameNode 不断检测DataNode是否有效,若失效，则寻找新的节点替代，将失效节点数据重新分布 
    * 集群负载均衡 
    * 数据一致性: 校验和checksum 
    * 主节点元数据失效 : Multiple FsImage and EditLog ,Checkpoint
4. HDFS文件系统操作命令 :见课件
* Hadoop MapReduce的基本工作原理 
1. 文件输入格式InputFormat : InputFormat提供了以下一些功能:
    * 选择文件或者其它对象，用来作为输入 
    * 定义InputSplits, 将一个文件分为不同任务 
    * 为RecordReader提供一个工厂，用来读取这个文件 
    * 当启动一个Hadoop任务的时候，一个输入文件所在的目录被输入到FileInputFormat对象中。 FileInputFormat从这个目录中读取所有文件。然后FileInputFormat将这些文件分割为多个InputSplits。 
    * 通过在JobConf对象上设置JobConf.setInputFormat设置文件输入的格式
    * 默认TextInputFormat：Key: 行偏移 Value: 该行内容
    * KeyValueTextInputFormat： Key: tab前的内容 Value:tab后的内容
2. Mapper : 每一个Mapper类的实例生成了 一个Java进程，负责处理某一个InputSplit上的数据 
3. Combiner : 合并相同key的键值对，减少partitioning时候的数据通信开销 
4. Partitioner & Shuffle : 用户可以提供一个Partitioner类， 用来决定一个给定的(key,value)对传给哪个Reduce节点;传输到每一个Reducer节点上的、将被所有的Reduce函数接收到的Key,value对会被Hadoop自动排序
5. 做用户定义的Reduce操作 
6. 文件输出格式OutputFormat : 每一个Reducer都写一个文件到一个共同的输出目录，文件名是 part-nnnnn，其中nnnnn是与每一个 reducer相关的一个号;TextOutputFormat : Default; writes lines in "key \t value" form
* 容错处理与计算性能优化 
1. 由Hadoop系统自己解决 
2. 主要方法是将失败的任务进行再次执行 
3. TaskTracker会把状态信息汇报给JobTracker，最终由JobTracker决定重新执行哪一个任务 
4. 为了加快执行的速度，Hadoop也会自动重复执行同一个任务，以最先执行成功的为准（投机执行） 


